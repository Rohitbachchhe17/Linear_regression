# Linear_regression Supervised learning 

Defination: Linear regression is a statistical technique to describe relationships between dependent variables with a number of independent variables. 

## Types of Linear Regression
There are two main types of linear regression:

# 1) Simple linear regression: 
This involves predicting a dependent variable based on a single independent variable.
# 2) Multiple linear regression: 
This involves predicting a dependent variable based on multiple independent variables.

# Applications of Linear Regression
1) Trend lines: A trend line represents the variation in quantitative data with the passage of time (like GDP, oil prices, etc.). These trends usually follow a linear relationship. Hence, linear regression can be applied to predict future values. However, this method suffers from a lack of scientific validity in cases where other potential changes can affect the data.
2) Economics: Linear regression is the predominant empirical tool in economics. For example, it is used to predict consumer spending, fixed investment spending, inventory investment, purchases of a countryâ€™s exports, spending on imports, the demand to hold liquid assets, labor demand, and labor supply.
3) Finance: The capital price asset model uses linear regression to analyze and quantify the systematic risks of an investment.
4) Biology: Linear regression is used to model causal relationships between parameters in biological systems.

## Advantages of Linear Regression
1)  Easy to interpret: The coefficients of a linear regression model represent the change in the dependent variable for a one-unit change in the independent variable, making it simple to comprehend the relationship between the variables.
2) Robust to outliers: Linear regression is relatively robust to outliers meaning it is less affected by extreme values of the independent variable compared to other statistical methods.
Can handle both linear and nonlinear relationships: Linear regression can be used to model both linear and nonlinear relationships between variables. This is because the independent variable can be transformed before it is used in the model.
3)No need for feature scaling or transformation: Unlike some machine learning algorithms, linear regression does not require feature scaling or transformation. This can be a significant advantage, especially when dealing with large datasets.

## Disadvantages of Linear Regression
1) Assumes linearity: Linear regression assumes that the relationship between the independent variable and the dependent variable is linear. This assumption may not be valid for all data sets. In cases where the relationship is nonlinear, linear regression may not be a good choice.
2) Sensitive to multicollinearity: Linear regression is sensitive to multicollinearity. This occurs when there is a high correlation between the independent variables. Multicollinearity can make it difficult to interpret the coefficients of the model and can lead to overfitting.
3) May not be suitable for highly complex relationships: Linear regression may not be suitable for modeling highly complex relationships between variables. For example, it may not be able to model relationships that include interactions between the independent variables.
4) Not suitable for classification tasks: Linear regression is a regression algorithm and is not suitable for classification tasks, which involve predicting a categorical variable rather than a continuous variable.

![image](https://github.com/Rohitbachchhe17/Linear_regression/assets/163370274/bc724300-b70f-473a-9cbf-624bc7e150cf)

